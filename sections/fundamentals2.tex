\subsection{Model specification and modeling assumptions}
\label{sec:modelspecificationandmodelingassumptions}

In this section, it is discussed how the specification of mixed models differs from that of fixed effects models, and that for each model with random effects there is an alternative models with only fixed effects.
A major focus is on the question of when to use fixed and random effects.
The amount of technicality and notation is kept at the absolute minimum.
Particularly, the specification of models in mathematical notation is not always shown, and models are introduced in \texttt{R} notation.
For an appropriate understanding of model specification, readers should consult a more in-depth text book, for example Part~2A of \citet{GelmanHill2006} (pp.~235--342).
Without any knowledge of the mathematical notation conventions, it is impossible to understand many advanced text books and much of the valuable advice available online.

\subsubsection{Simple random intercepts}
\label{sec:simplerandomintercepts}

Readers with experience in fixed effects modeling should be able to see that a grouping factor encoding the verb lemma and all the other potential grouping factors discussed in the previous sections could be specified as normal fixed effects in a GLM.
This section introduces the main difference between the fixed-effect approach and the random-effect approach.
Logistic regression examples are used throughout this section, and we begin with the fictional corpus study of the dative alternation introduced in Sections~\ref{sec:hierarchicalormultilevelmodels} and \ref{sec:randominterceptsandslopes}.
We focus only on model specification here, and hence the full \texttt{R} commands including the specification of the link function and the distribution family are not shown.
They are always assumed to be the logit link (\ie\ the inverse logit function) and the binomial distribution in the examples.

First, we specify a minimal model as (\ref{eq:001}) with only the \textit{Lemma} grouping factor and one other (binary) predictor, namely \textit{Givenness}, both as fixed effects.

\Rformula{Construction}{1+Lemma+Givenness}{eq:001}

In the case of logistic regression in alternation modelling, \textit{Construction} is binary (levels 0 or 1, corresponding to the two alternants).
Furthermore, \textit{Lemma} has $m$ levels (one for each lemma), and \textit{Givenness} is also binary (levels 0 and 1, corresponding to \textit{not given} and \textit{given}).
A statement like (\ref{eq:001}) encodes a theoretical commitment to what the researcher thinks is the mechanism that determines which alternant is chosen.
Concretely, it encodes the assumption that the probability of the outcome which is labeled as 1 can be predicted from the additive linear term specified as \textit{1+Lemma+Givenness}.
Because the influence of the regressors on the outcome is not linear in many cases, the additive linear term is transformed through the link function (here assumed to be the inverse logit function), which is not encoded directly in \texttt{R}-type model formulæ.
Also not part of the model formula in \texttt{R} is the specification of the distribution of the residuals (assumed to be binomial), which encodes the assumption that the distribution of the prediction errors follows the binomial distribution.%
\footnote{As the example is still a GLM, this is merely a recapitulation of the previous chapter.}
If another distribution (such as the Poisson distribution) and another link function (such as the logarithm, which is the default for Poisson models) is chosen, the specification in (\ref{eq:001}) remains the same.

In any type of GL(M)M, the additive linear term consists of a number of sub-terms which are simply added up.
Each of these sub-terms (except for the intercepts) consists of the multiplication of the (estimated) \textit{coefficient} with an observed \textit{value} of one of the variables.
However, \texttt{R} notation for model formulæ simplifies the specification of the actual linear term.
First of all, the 1 in (\ref{eq:001}) is \texttt{R}'s way of encoding the fact that an \textit{intercept} is part of the model.
An intercept is a constant sub-term to which all other sub-terms are added, and it can be seen as the reference value when all other sub-terms (corresponding to categorical or numeric regressors) assume 0 as their value.

For binary regressors like \textit{Givenness}, the only coefficient that is estimated directly encodes the value added to (in case of a positive coefficient) or subtracted from (in case of a negative coefficient) the linear term when the value of the regressor is 1 (in the example, when the referent is given).
When the value of the regressor is 0 (for example, when the referent is not given), 0 is added to the intercept.
The intercept thus encodes (among other things) something like a default for a binary regressor.
If the default corresponds to, as in the example, non-givenness, phrases like ``non-givenness is on the intercept'' or ```givenness equals zero' is on the intercept'' are often used.

\begin{table}
  \centering
  \begin{tabular}{cccc}
    \toprule
    \multicolumn{4}{l}{\textbf{Value of\ldots}} \\
    \textbf{Lemma} & $\mathbf{l_1}$ & $\mathbf{l_2}$ & $\mathbf{l_3}$ \\
    \midrule
    1 & 0 & 0 & 0 \\ 
    2 & 1 & 0 & 0 \\ 
    3 & 0 & 1 & 0 \\ 
    4 & 0 & 0 & 1 \\ 
    \bottomrule
  \end{tabular}
  \caption{Dummy coding of a categorical variable \textit{Lemma} with four levels, resulting in the binary dummy variables $l_1,l_2,l_3$}
  \label{tab:dummy}
\end{table}

However, a grouping factor such as \textit{Lemma} is usually a categorical variable with more than two levels.
In such a case, the $m$ levels of the grouping factor are \textit{dummy-coded}, and for all but one of these binary dummy variables, a coefficient is estimated.
Dummy coding is a way of encoding a categorical variable as a number of binary variables, see Table~\ref{tab:dummy}, and \texttt{R} takes care of dummy-coding automatically.
Because the first of the $m$ levels of the grouping factor is encoded by all dummy variables assuming the value 0, only $m-1$ sub-terms are added to the model, which means that only $m-1$ coefficients have to be estimated.
The first level of the grouping factor is thus ``on the intercept'' and becomes the reference to which all other levels are compared.%
\footnote{Picking one dummy as a reference level is necessary because otherwise infinitely many equivalent estimates of the model coefficients exist as one could simply add any arbitrary constant to the intercept and shift the other coefficients accordingly.
However, the estimator works under the assumption that there is a unique maximum likelihood estimate.
This extends to any other appropriate coding for categorical grouping variables.
}

To sum up and clarify, if in a given study \textit{Lemma} has four levels dummy-coded as $l_1$, $l_2$, $l_3$, and \textit{Givenness} is binary and coded as $g$ with $g=1$ if the referent is given, the formula corresponding to (\ref{eq:001}) looks like (\ref{eq:001a}) in mathematical notation, where the linear additive term is enclosed in $[ ]$, each sub-term appears in $( )$, and $c$ encodes the choice of the two alternants.

\begin{equation}
  Pr(c=1)=logit^{-1}\left[\alpha_0+(\beta_{l_1}\cdot l_1)+(\beta_{l_2}\cdot l_2)+(\beta_{l_3}\cdot l_3)+(\beta_{g}\cdot g)\right]
  \label{eq:001a}
\end{equation}

In plain English: the probability $Pr$ that the alternant of the construction coded as $1$ is chosen $Pr(c=1)$ is calculated as the inverse logit of the linear term.
The linear term is just the sum of the intercept $\alpha_0$ and the measured values, each multiplied by its corresponding coefficient labelled $\beta$.

In such a model, the effect of each verb lemma is treated as a fixed population parameter, just like the effect of givenness.
In other words, the algorithm which estimates the coefficients for the $m-1$ dummy variables tries to find a fixed value for each of them without taking the variation between them into account.
With many levels, this requires a lot of data, and levels for which only a few observations are available in the data set have very imprecise coefficient estimates with large confidence intervals.

This is where random effects come into play as an alternative.
If we treat the same grouping factor as a random intercept, we let the intercept vary by group, \ie\ each group is allowed to have its own intercept.
Furthermore, we give the varying intercepts a (normal) distribution instead of estimating $m-1$ fixed population parameters.
This means that the group-wise intercepts are assumed to be normally distributed around $0$.
This and nothing else is the crucial difference between a fixed effect and a random effect.

In \texttt{R}, the model specification then looks like (\ref{eq:002}), where ``1|'' can be read as ``an intercept varying by''.

\Rformula{Construction}{1+Givenness+(1|Lemma)}{eq:002}

The sub-term \textit{Givenness} remains the same as in (\ref{eq:001}), and it is still treated as a fixed effect.
The sub-term \textit{(1|Lemma)} encodes that an intercept will be added to the linear term depending on which lemma is observed.
Notice that the sub-term for the varying intercept (just like the one for the normal intercept) does not involve multiplication.
This is obvious in mathematical notation corresponding to (\ref{eq:002}) as shown in (\ref{eq:002a}).
In addition to the overall intercept $\alpha_0$, there is another constant term $\alpha_{[Lemma]}$, which is chosen appropriately for each level of \textit{Lemma}.

\begin{equation}
  Pr(c=1)=logit^{-1}\left[\alpha_0+\alpha_{[Lemma]}+(\beta_{g}\cdot g)\right]
  \label{eq:002a}
\end{equation}

Crucially, instead of estimating a batch of $m-1$ coefficients for the levels of the grouping variable, a varying intercept (assumed to come from a normal distribution) is predicted for each of its $m$ levels.
Essentially, this means that the varying intercepts are predicted from their own linear model.
All more complex model structures to be discussed below are extensions of this approach.

\subsubsection{Random effect or fixed effect}
\label{sec:choosingbetweenrandomandfixedeffects}

One commonly given reason to use a random effect instead of a fixed effect is that ``the researcher is not interested in the individual levels of the random effect'' (or variations thereof).
Such recommendations should be taken with a grain of salt.
\citet[245--247]{GelmanHill2006} summarise this as well as other diverging and partially contradicting recommendations for what should be a random effect as found in the literature.
They conclude that there is essentially no universally accepted and tenable conceptual criterion of deciding what should be a random effect and what a fixed effect.
The author of this chapter agrees with their conclusion that random effects should be preferred whenever it is technically feasible.
Understanding when it is technically feasible requires at least some understanding of two major points.
First, the variance in the intercepts needs to be estimated if a random effect is used.
Second, the random intercepts can be understood as a compromise between fitting separate models for each group of the grouping factor (\textit{no pooling}) and fitting a model while ignoring the grouping factor altogether (\textit{complete pooling}), see \citet[Ch.~12]{GelmanHill2006}.

As was stated above, the random intercepts are assumed to come from a normal distribution, and therefore the variance between them has to be estimated with sufficient precision.
From the estimated variance and the data for a specific group, the estimator predicts the \textit{conditional mode} in a GLMM or the \textit{conditional mean} in a LMMs for that group (see \citealt[Ch.~1]{Bates2010}).
The conditional mode\slash mean for a group is the value of the varying intercept for this group.
It is the numerical value shown by \texttt{R} packages like \texttt{lme4} for each level of a random intercept variable.
This procedure, however, requires that the number of groups must not be too low.
As a rule of thumb, if there are fewer than five levels, a grouping factor should be included as a fixed effect, regardless of its conceptual interpretation.
Although one often finds default recommendations telling practitioners to use a speaker grouping variable as a random effect, it would be ill-advised to do so if there are exemplars from less than five speakers in the sample.
Along the same lines, the distinction between spoken and written is not a suitable grouping factor for use as a random effect because it has too few levels.

If, however, the number of groups is reasonably large, the next thing to consider is the number of observations per group.
Alternatives to using a random effect would be to estimate a separate model for each level of the grouping factor, or to include it as a fixed effect.
In both cases the effects are not treated as random variables, and fixed coefficients per group are estimated without taking the between-group variance into account.
With a random effect, however, the conditional modes\slash means are pulled (\textit{shrunken}) towards the overall intercept (\textit{shrinkage}).
When the number of observations in a group is low, the conditional mode\slash mean is simply shrunken more strongly towards $0$, predicting only a small deviation from the overall tendency.%
\footnote{Terminologically, shrinkage is thus \textit{stronger} (and the conditional mode\slash mean is closer to $0$) if there is less evidence that a group deviates from the overall tendency.
The lower the number of observations per group, the lesser evidence there is.}
On the other hand, fixed effect estimates would become inexact and would probably be dismissed because of growing uncertainty in the estimate (large confidence intervals, high p-values) when the number of observations for a level is low.
Thus, low numbers of observations in all or some groups are often detrimental for using fixed effects grouping factors.
Random effects are much more robust in such situations because of shrinkage.
On the downside, a conditional mode that was strongly shrunken (due to a low number of observations) cannot be distinguished straightforwardly from a conditional mode of a group which simply does not deviate a lot from the average tendency.
For fixed effects, we have both a parameter estimate and a possible significance test, but for random effects, we only have the prediction of the conditional mode\slash mean.
However, so-called \textit{prediction intervals} can be calculated for individual per-group intercepts, and we return to them in the following section.


\subsubsection{Model quality and model selection}
\label{sec:significancetestingandcoefficientsofdetermination}

\paragraph{Significance}

It is not adequate to do any kind of significance testing on the individual levels of the random effect because they are not estimates in the conceptual and technical sense.%
\footnote{Again, we do not assume them to be fixed population parameters, which would be the case for true estimates such as fixed effects coefficients.}
There are ways of calculating \textit{prediction intervals} (which are not the same as confidence intervals) for conditional modes in order to specify the quality of the fit (see Section~\ref{sec:practicalguidewithr}), but they should not be misused for talking about significance.
Not doing significance tests for single levels of the grouping factor does, however, not mean that the researcher is not interested in the individual conditional modes, which is proven by the fact that they are often reproduced in research papers, for example in the form of a dot plot.
Also, we can still get a good idea of the per-group tendencies by looking at the conditional modes\slash means.
Additionally, a random effect allows the researcher to quantify the between-group variance, which is not possible for fixed effects.

\paragraph{Model selection}

A related question is \textit{model selection}, \ie whether the inclusion of the random effect improves the model quality.
It is recommended here to include all conceptually necessary random effects and only remove them if they clearly (and beyond doubt) have no effect.
To check whether they have an effect, the estimated between-group variance is the first thing to look at.
If it is close to $0$, there is most likely not much going on between groups, or there simply was not enough data to estimate the variance.
In LMMs, it is possible to compare the residual (item-level) variance with the between-group variance to see which one is larger, and to which degree.
If, for example, the residual variance is 0.2 and the between-group variance is 0.8, then we can say that the between-group variance is four times larger than the residual variance, which would indicate that the random effect has a considerable impact on the response.
This comparison is impossible in GLMMs because their (several types of) residuals do not have the same straightforward interpretation as those of LMMs.

Furthermore, models can be compared using likelihood ratio (LR) tests.
In such tests, a model including the random effect and a model not including it are compared, similar to LR tests for the comparison of fixed effects.
Such pairs of models, where one is strictly a simplification of the other, are called \textit{nested models} (not to be confused with \textit{nested effects} discussed in Section~\ref{sec:crossedandnestedeffects}).
A sometimes more robust alternative to the ordinary LR test are parametric bootstrap tests. 

With all this, it should be kept in mind that it is \textit{never} appropriate to make formal comparisons between a GLMM with a random effect and a GLM with the same factor as a fixed effect using any test or metric (including so-called information criteria such as Akaike's or Bayes').
\textit{Informally} comparing coefficients of determination ($R^2$) between such pairs of models has a limited use, as will be shown below.

\paragraph{Quality of the fit}

To measure how well a GLMM fits the data, any metric that is based on prediction accuracy can be used in the same way as with GLMs.
For example, the rate of correct predictions on the data used for model estimation or cross-validation methods are appropriate.

Coefficients of determination (pseudo-$R^2$) can be used to give some idea of the overall model fit.
For GLMMs, \citet{NakagawaSchielzeth2013} have proposed a method that distinguishes between \textit{marginal} $R^2$ (only fixed effects) and \textit{conditional} $R^2$ (fixed and random effects).
This has become a de facto standard.
In cases where an effect works well as a fixed or a random effect (for example, if it has between five and ten levels with enough data points for each level), the marginal and conditional $R^2$ measures for the GLMM converge in an expected way with Nagelkerke's $R^2$ for corresponding GLMs.
The marginal $R^2$ for a GLMM estimate is roughly the same as Nagelkerke's $R^2$ for a GLM estimate where the grouping factor is ignored.
Also, the conditional $R^2$ for a GLMM estimate is roughly the same as Nagelkerke's $R^2$ for a GLM estimate which includes the grouping factor as a fixed effect.


\subsubsection{More complex models}
\label{sec:morecomplexmodels}

\paragraph{Varying intercepts and slopes}

In Section~\ref{sec:randominterceptsandslopes}, it was shown under which conditions a varying-intercept and varying-slope (VIVS) model might be useful.
Readers might want to review the example before continuing on.
While it is possible to have just a varying slope, this is rarely useful, and we discuss only varying-intercept and varying-slope (VIVS) models.

A random slope is a good choice when the strength or direction of some fixed effect varies by group.
We extend the simple model from (\ref{eq:001}) to include random slopes for \textit{Givenness} varying by \textit{Lemma} using \texttt{R} notation in (\ref{eq:003}).
Each variable from the fixed effects part of the formula which we expect to vary by \textit{Lemma} is simply repeated before the | symbol.

\Rformula{Construction}{1+Givenness+(1+Givenness|Lemma)}{eq:003}

With this model specification, a fixed coefficient for \textit{Givenness} will still be estimated.
However, an additional value will be predicted for each lemma, and this value has to be added to the fixed coefficient.
In mathematical notation, this is very transparent, as shown in (\ref{eq:003a}).
The varying slope for \textit{Givenness} to be chosen appropriately for each \textit{Lemma} is specified as $\beta_{g[Lemma]}$.

\begin{equation}
  Pr(c=1)=logit^{-1}\left[\alpha_0+\alpha_{[Lemma]}+((\beta_{g}+\beta_{g[Lemma]}) \cdot g)\right]
  \label{eq:003a}
\end{equation}

A source of problems in VIVS models is the fact that in addition to the variance in the intercepts and slopes, the covariance between them has to be estimated.
If in groups with a higher-than-average intercept, the slope is also higher than average, they are positively correlated, and vice versa.
These relations are captured in the covariance.
Technically speaking, the joint distribution of the random intercepts and the random slopes is assumed to follow a bivariate normal distribution with means, variances, and covariances to be estimated.
The number of variance parameters to be estimated thus obviously increases with more complex model specifications, and the estimation of the parameters in the presence of complex variance-covariance matrices requires considerably more data than estimating a single variance parameter.
The estimator algorithm might terminate, but typically covariance estimates of $-1$ or $1$ indicate that the data was too sparse for a successful estimation of the parameter.
In this case, the model is \textit{over-parametrised} and needs to be simplified (see \citealt{BatesEa2015a,MatuschekEa2017}).

\paragraph{Nested and crossed random effects}

The difference between nested and crossed random effects is only defined when there are two or more random effects.
As it was explained in Section~\ref{sec:crossedandnestedeffects}, nested random effects are appropriate tools when the levels of a grouping factor are nested within the levels of another grouping factor.
Technically, while varying slopes can be understood as interactions between a fixed and a random effect, nested random intercepts can be understood as interactions between two or more random effects.
Crossed random effects are just several unrelated random effects.

In the model specification (both in \texttt{R} notation and in mathematical notation), there is no difference between a crossed and a nested random effect.
Both are specified like independent random effects.
See (\ref{eq:004}) for an example in \texttt{R} notation which extends (\ref{eq:002}).

\Rformula{Construction}{1+Givenness+(1|Lemma)+(1|Semantics)}{eq:004}

In (\ref{eq:004}), \textit{Semantics} could be a factor encoding the semantic classes which nest individual lemmas.
It could also be a (crossed) grouping factor completely unrelated to the lemmas, for example encoding some semantic property of the whole sentence containing the construction.
As was mentioned in Section~\ref{sec:crossedandnestedeffects}, the question on the practitioner's side is rather how the data are organised.
If the data have a nested structure, the estimator will treat them as nested, otherwise as crossed.
Data have a nested structure whenever each level of a (nesting) random effect can always be determined entirely from the levels of another (nested) factor.


\paragraph{Second-level predictors}

In Section~\ref{sec:hierarchicalormultilevelmodels}, situations were introduced where the random effects themselves can be partially predicted from second-level fixed-effects.
In this case, an additional linear (Gaussian) model is used to predict the random effects.
In \texttt{R} notation, the true model structure is entirely blurred, and practitioners even run the risk of working with second-level predictors without realising it.

We extend (\ref{eq:002}) to (\ref{eq:005}) by adding a numeric fixed effect which specifies the token frequency for each level of \textit{Lemma}.

\Rformula{Construction}{1+Givenness+Lemmafrequency+(1|Lemma)}{eq:005}

This is the only way to specify second-level predictors in standard \texttt{R} notation.
The data set has to be organised as shown in Table~\ref{tab:multilevel}, where for each data point a level of \textit{Lemma} is specified and the appropriate frequency value for this level of \textit{Lemma} is given in a separate column.%
\footnote{There are, of course, elegant ways of pulling the frequency values from another data frame on the fly in \texttt{R}.}
\texttt{R} treats \textit{Lemmafrequency} as a second-level predictor automatically under such conditions.
Simply speaking, this means that the random intercept for \textit{Lemma} will now be predicted from its own linear model.
For illustration purposes only, such a second-level linear model is specified here as (\ref{eq:005a}).%
\footnote{Users do not have to specify this formula.}

\Rformula{Lemmaintercept}{1+Lemmafrequency}{eq:005a}

The random effect (a random intercept in this case) is thus broken down into a second-level intercept (denoted by 1 in \texttt{R}) and a second-level fixed effect (in this case \textit{Lemmafrequency}).
In Section~\ref{sec:practicalguidewithr}, a model with second-level effects will be used for illustration purposes.


